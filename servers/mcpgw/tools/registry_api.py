"""
MCP Gateway Registry API Tools
- execute_tool: Execute tools from any MCP server
- read_resource: Read/access resources from any MCP server
- execute_prompt: Execute prompts from any MCP server
"""

import logging
from collections.abc import Callable
from typing import Any

from core.registry import call_registry_api
from fastmcp import Context
from pydantic import Field

logger = logging.getLogger(__name__)


async def execute_tool_impl(
    server_path: str,
    tool_name: str,
    arguments: dict[str, Any],
    server_id: str | None = None,
    ctx: Context = None,
) -> dict[str, Any]:
    """
    Execute a specific tool (implementation layer).

    Args:
        server_path: Server path from discovery (e.g., '/tavilysearch')
        tool_name: Resolved tool name to send to MCP server (e.g., 'tavily_search')
        arguments: Tool-specific arguments as key-value pairs
        server_id: Server ID from discovery (e.g., '6972e222755441652c23090f')
        ctx: FastMCP context with user auth

    Returns:
        Tool execution result

    Note:
        This is the implementation layer - tool_name should already be resolved
        by the interface layer (execute_tool). This function just sends the
        request to the registry API.
    """
    logger.info(
        f"ðŸ”§ Executing tool: {tool_name} on {server_path}"
        + (f" (server_id: {server_id})" if server_id else "")
    )

    try:
        # Build request payload
        payload = {"server_path": server_path, "tool_name": tool_name, "arguments": arguments}
        if server_id:
            payload["server_id"] = server_id

        # Use centralized registry API call with automatic auth header extraction
        result = await call_registry_api(
            method="POST", endpoint="/proxy/tools/call", ctx=ctx, json=payload
        )

        logger.info(f"âœ… Tool execution successful: {tool_name}")
        return result

    except Exception as e:
        logger.error(f"âŒ Tool execution failed: {e}")
        raise Exception(f"Tool execution failed: {e!s}")


async def read_resource_impl(
    server_id: str, resource_uri: str, ctx: Context = None
) -> dict[str, Any]:
    """
    Read/access a resource from an MCP server.

    Args:
        server_id: Server ID from discovery
        resource_uri: Resource URI to read (e.g., 'tavily://search-results/AI')
        ctx: FastMCP context with user auth

    Returns:
        Resource contents (text, JSON, binary, etc.)

    Example:
        result = await read_resource_impl(
            server_id="6972e222755441652c23090f",
            resource_uri="tavily://search-results/AI"
        )
    """
    logger.info(f"ðŸ“„ Reading resource: {resource_uri} from server {server_id}")

    try:
        # Build request payload
        payload = {"server_id": server_id, "resource_uri": resource_uri}

        # Use centralized registry API call with automatic auth header extraction
        result = await call_registry_api(
            method="POST", endpoint="/proxy/resources/read", ctx=ctx, json=payload
        )

        logger.info(f"âœ… Resource read successful: {resource_uri}")
        return result

    except Exception as e:
        logger.error(f"âŒ Resource read failed: {e}")
        raise Exception(f"Resource read failed: {e!s}")


async def execute_prompt_impl(
    server_id: str, prompt_name: str, arguments: dict[str, Any] | None = None, ctx: Context = None
) -> dict[str, Any]:
    """
    Execute a prompt from an MCP server.

    Args:
        server_id: Server ID from discovery
        prompt_name: Name of the prompt to execute
        arguments: Optional prompt arguments
        ctx: FastMCP context with user auth

    Returns:
        Prompt messages ready for LLM consumption

    Example:
        result = await execute_prompt_impl(
            server_id="6972e222755441652c23090f",
            prompt_name="research_assistant",
            arguments={
                "topic": "Artificial Intelligence",
                "depth": "comprehensive"
            }
        )
    """
    logger.info(f"ðŸ’¬ Executing prompt: {prompt_name} on server {server_id}")

    try:
        # Build request payload
        payload = {"server_id": server_id, "prompt_name": prompt_name, "arguments": arguments or {}}

        # Use centralized registry API call with automatic auth header extraction
        result = await call_registry_api(
            method="POST", endpoint="/proxy/prompts/execute", ctx=ctx, json=payload
        )

        logger.info(f"âœ… Prompt execution successful: {prompt_name}")
        return result

    except Exception as e:
        logger.error(f"âŒ Prompt execution failed: {e}")
        raise Exception(f"Prompt execution failed: {e!s}")


# ============================================================================
# Tool Factory Functions for Registration
# ============================================================================


def get_tools() -> list[tuple[str, Callable]]:
    """
    Export tools for registration in server.py.

    Returns:
        List of (tool_name, tool_function) tuples ready for registration
    """

    # Define tool wrapper function with proper signature and decorators
    async def execute_tool(
        server_path: str = Field(
            ..., description="Server path from discovery (e.g., '/tavilysearch' for web search)"
        ),
        tool_name: str = Field(
            ...,
            description="Tool name from discovery - can be scoped name (e.g., 'tavily_search_mcp_tavily_search')",
        ),
        arguments: dict[str, Any] = Field(..., description="Tool parameters from input_schema"),
        server_id: str = Field(..., description="Server ID from discovery"),
        mcp_tool_name: str | None = Field(
            None,
            description="Original MCP tool name (e.g., 'tavily_search') - extract from toolFunction.mcpToolName if available",
        ),
        ctx: Context | None = None,
    ) -> dict[str, Any]:
        """
        ðŸš€ AUTO-USE: Execute any discovered tool to get real-time data.

        **Common Examples:**
        ```
        # Web search
        execute_tool(
            server_path="/tavilysearch",
            tool_name="tavily_search",
            mcp_tool_name="tavily_search",  # Extract from toolFunction.mcpToolName
            arguments={"query": "latest AI news", "max_results": 5},
            server_id="6972e222755441652c23090f"
        )

        # GitHub operations
        execute_tool(
            server_path="/github",
            tool_name="search_pull_requests",
            arguments={"owner": "org", "repo": "project", "state": "open"},
            server_id="abc123..."
        )
        ```

        **Parameters:**
        - server_path: From discovery (e.g., "/tavilysearch")
        - tool_name: Tool identifier (can be scoped name like "tavily_search_mcp_tavily_search")
        - mcp_tool_name: Original MCP name from toolFunction.mcpToolName (e.g., "tavily_search")
        - arguments: Tool-specific parameters from input_schema
        - server_id: Server ID from discovery

        **Note:** If mcp_tool_name is provided, it's used for execution (MCP standard).
        Otherwise, tool_name is used directly.

        âš ï¸ Use after discover_servers to execute tools.
        Returns: Tool-specific results (format varies by tool)
        """
        # Resolve tool name at interface layer: use mcpToolName if provided, otherwise use tool_name
        resolved_tool_name = mcp_tool_name or tool_name

        # Log the resolution for debugging
        if mcp_tool_name and mcp_tool_name != tool_name:
            logger.debug(
                f"Resolved tool name: '{resolved_tool_name}' (from mcpToolName, scoped was '{tool_name}')"
            )

        return await execute_tool_impl(server_path, resolved_tool_name, arguments, server_id, ctx)

    async def read_resource(
        server_id: str = Field(
            ..., description="Server ID from discover_servers (e.g., '6972e222755441652c23090f')"
        ),
        resource_uri: str = Field(
            ...,
            description="Resource URI to read (e.g., 'tavily://search-results/AI', 'file:///path/to/data')",
        ),
        ctx: Context | None = None,
    ) -> dict[str, Any]:
        """
        ðŸ“„ Read/access resources from any MCP server.

        **What are resources?**
        Resources are data sources, caches, URIs, or file-like objects exposed by MCP servers.
        Examples: cached search results, configuration files, data streams, API responses.

        **Common use cases:**
        - Access cached data: read_resource(server_id="...", resource_uri="tavily://search-results/AI")
        - Read configuration: read_resource(server_id="...", resource_uri="config://settings")
        - Access files: read_resource(server_id="...", resource_uri="file:///data/export.json")

        **Workflow:**
        1. Use discover_servers to find servers with resources
        2. Review the resources array in server config
        3. Use read_resource with the resource URI

        **Example:**
        ```
        # Discover servers with resources
        servers = discover_servers(query="search")

        # Find a resource URI from server config
        resource = servers[0]["config"]["resources"][0]["uri"]

        # Read the resource
        data = read_resource(
            server_id=servers[0]["_id"],
            resource_uri=resource
        )
        ```

        Returns: Resource contents (format varies: text, JSON, binary, etc.)
        """
        return await read_resource_impl(server_id, resource_uri, ctx)

    async def execute_prompt(
        server_id: str = Field(..., description="Server ID from discover_servers"),
        prompt_name: str = Field(
            ...,
            description="Name of the prompt to execute (e.g., 'research_assistant', 'fact_checker')",
        ),
        arguments: dict[str, Any] | None = Field(
            None, description="Prompt arguments as key-value pairs"
        ),
        ctx: Context | None = None,
    ) -> dict[str, Any]:
        """
        ðŸ’¬ Execute prompts from any MCP server.

        **What are prompts?**
        Prompts are pre-configured, reusable prompt templates provided by MCP servers.
        They help standardize complex workflows and provide expert guidance.

        **Common use cases:**
        - Research workflows: execute_prompt(server_id="...", prompt_name="research_assistant", arguments={"topic": "AI"})
        - Fact checking: execute_prompt(server_id="...", prompt_name="fact_checker", arguments={"claim": "..."})
        - Code review: execute_prompt(server_id="...", prompt_name="code_reviewer", arguments={"language": "python"})

        **Workflow:**
        1. Use discover_servers to find servers with prompts
        2. Review the prompts array in server config
        3. Use execute_prompt with required arguments

        **Example:**
        ```
        # Discover servers with prompts
        servers = discover_servers(query="research")

        # Find available prompts
        prompts = servers[0]["config"]["prompts"]

        # Execute a prompt
        result = execute_prompt(
            server_id=servers[0]["_id"],
            prompt_name="research_assistant",
            arguments={"topic": "Quantum Computing", "depth": "advanced"}
        )

        # Result contains messages ready for LLM
        messages = result["messages"]
        ```

        Returns: Prompt messages ready for LLM consumption (role, content pairs)
        """
        return await execute_prompt_impl(server_id, prompt_name, arguments, ctx)

    # Return list of (name, function) tuples
    return [
        ("execute_tool", execute_tool),
        ("read_resource", read_resource),
        ("execute_prompt", execute_prompt),
    ]
